# ğŸ—‚ï¸ğŸ“Š Big Data Erweiterungen: Aufgabe 4

> **Cloud Computing und Big Data - Portfolio-PrÃ¼fung**  
> Erweiterte Implementierung mit Data Lake, Stream Processing und ML

---

## ğŸ—ï¸ **Erweiterte Infrastruktur mit Big Data Stack**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OpenStack Cloud                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ K8s Master  â”‚    â”‚ K8s Worker1 â”‚ â”‚ K8s Worker2 â”‚ â”‚ Ingressâ”‚  â”‚
â”‚  â”‚             â”‚    â”‚             â”‚ â”‚             â”‚ â”‚        â”‚  â”‚
â”‚  â”‚ - K3s       â”‚â—„â”€â”€â”€â”¤ - K3s Agent â”‚ â”‚ - K3s Agent â”‚ â”‚ Traefikâ”‚  â”‚
â”‚  â”‚ - Docker    â”‚    â”‚ - Docker    â”‚ â”‚ - Docker    â”‚ â”‚        â”‚  â”‚
â”‚  â”‚ - Prometheusâ”‚    â”‚ - App Pods  â”‚ â”‚ - App Pods  â”‚ â”‚ :80    â”‚  â”‚
â”‚  â”‚ - Grafana   â”‚    â”‚ - HPA       â”‚ â”‚ - HPA       â”‚ â”‚        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚      Zero-Downtime Deployment with Terraform Workspaces     â”‚â”‚
â”‚  â”‚      Blue-Green Strategy + Health Checks + Auto-Rollback    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚       Big Data Stack (Namespace: big-data)                  â”‚â”‚
â”‚  â”‚     - MinIO Data Lake (S3-kompatibel): :30900, :30901       â”‚â”‚
â”‚  â”‚     - Python ML Jobs (scikit-learn): Batch-Processing       â”‚â”‚
â”‚  â”‚     - Persistente Daten: raw-data, processed-data Buckets   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### **Big Data Tech Stack**

| Komponente | Technologie | Zweck | Integration |
|------------|-------------|-------|------------|
| **Object Storage** | MinIO | S3-kompatibel Data Lake | NodePort 30900, 30901 |
| **ML Processing** | Python scikit-learn | Batch ML Jobs auf K8s | K8s Job-Objekte |
| **Data Formats** | CSV, JSON, Parquet | Raw & Processed Data | MinIO Buckets |
| **Model Types** | Random Forest | Vorhersagemodelle | ML Pipeline |

## ğŸ¯ **Ãœberblick der Erweiterungen**

### **Aufgabe 4: Data Lake / Big Data-Processing** âœ… **VOLLSTÃ„NDIG IMPLEMENTIERT**
- âœ… **Verteilter Data Lake**: MinIO (S3-kompatibel) fÃ¼r Object Storage
- âœ… **Big Data Processing**: Python ML Jobs auf Kubernetes
- âœ… **Machine Learning**: scikit-learn fÃ¼r Food Calorie Analysis
- âœ… **Bonus Features**: Cloud-native Stack, horizontale Skalierung

---

## ğŸš€ **Quick Start fÃ¼r Aufgabe 4**

### **Setup Data Lake & Batch Processing**
```bash
# âš ï¸  WICHTIG: Befehle in dieser Reihenfolge ausfÃ¼hren!

# 1. Data Lake installieren (ZUERST - erstellt big-data namespace + MinIO)
./version-manager.sh setup-datalake

# 2. ML Pipeline starten (benÃ¶tigt setup-datalake!) 
./version-manager.sh ml-pipeline

# 3. Status prÃ¼fen (SSH erforderlich)
ssh -i ~/.ssh/$ssh_key ubuntu@$master_ip kubectl get pods -n big-data
ssh -i ~/.ssh/$ssh_key ubuntu@$master_ip kubectl get jobs -n big-data
```