# ğŸš€ Kubernetes Multi-Node Cluster mit Zero-Downtime Deployments

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Terraform](https://img.shields.io/badge/Terraform-1.0+-blue.svg)](https://terraform.io)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-K3s-326CE5.svg)](https://k3s.io)
[![OpenStack](https://img.shields.io/badge/OpenStack-Cloud-red.svg)](https://openstack.org)
[![Datalake](https://img.shields.io/badge/Datalake-MinIO-purple.svg)](https://www.min.io/)
[![Apache Spark (Spark MLlib)](https://img.shields.io/badge/Batch_Verarbeitung-SparkMLlib-red.svg)](https://spark.apache.org/mllib/)
[![Apache Kafka](https://img.shields.io/badge/Stream_Processing-ApacheKafka-231F20.svg)](https://kafka.apache.org/)

> **Ein vollstÃ¤ndiges Cloud Computing Projekt mit Immutable Infrastructure, Multi-Node Kubernetes, Zero-Downtime Deployments, Ingress Controller und KI-basierter Streamlit-Anwendung**

## ğŸ“– Ãœberblick

Dieses Projekt implementiert eine **Immutable Infrastructure** auf OpenStack mit einem Multi-Node Kubernetes-Cluster, der eine KI-gestÃ¼tzte Kalorien-SchÃ¤tzungs-App mit Streamlit und Google Gemini API hostet.

### ğŸ¯ Projektumfang

Das Projekt erfÃ¼llt **alle Anforderungen** der Portfolio-PrÃ¼fung "Cloud Computing und Big Data" und geht darÃ¼ber hinaus:

- âœ… **Aufgabe 1**: Immutable Infrastructure mit Terraform
- âœ… **Aufgabe 2**: Configuration Management und Deployment-Versionierung  
- âœ… **Aufgabe 3**: Multi-Node Kubernetes-Architektur mit skalierbarer Anwendung
- âœ… **Aufgabe 4**: Data Lake / Big Data-Processing

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OpenStack Cloud                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ K8s Master  â”‚    â”‚ K8s Worker1 â”‚ â”‚ K8s Worker2 â”‚ â”‚ Ingressâ”‚  â”‚
â”‚  â”‚             â”‚    â”‚             â”‚ â”‚             â”‚ â”‚        â”‚  â”‚
â”‚  â”‚ - K3s       â”‚â—„â”€â”€â”€â”¤ - K3s Agent â”‚ â”‚ - K3s Agent â”‚ â”‚Traefik â”‚  â”‚
â”‚  â”‚ - Docker    â”‚    â”‚ - Docker    â”‚ â”‚ - Docker    â”‚ â”‚        â”‚  â”‚
â”‚  â”‚ - Prometheusâ”‚    â”‚ - App Pods  â”‚ â”‚ - App Pods  â”‚ â”‚ :80    â”‚  â”‚
â”‚  â”‚ - Grafana   â”‚    â”‚ - HPA       â”‚ â”‚ - HPA       â”‚ â”‚        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚     Zero-Downtime Deployment with Terraform Workspaces      â”‚â”‚
â”‚  â”‚     Blue-Green Strategy + Health Checks + Auto-Rollback     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ External Access:     â”‚
               â”‚ â€¢ HTTP: :80 (Ingress)â”‚
               â”‚ â€¢ NodePort: :30001   â”‚
               â”‚ â€¢ Grafana: :30300    â”‚
               â”‚ â€¢ Prometheus: :30090 â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ› ï¸ Technology Stack

| Komponente | Technologie | Zweck | Bonus-Feature |
|------------|-------------|-------|---------------|
| **Infrastructure as Code** | Terraform | Immutable Infrastructure Management | âœ… Workspaces fÃ¼r Zero-Downtime |
| **Container Orchestration** | Kubernetes (K3s) | Multi-Node Cluster Management | âœ… Lightweight Production K8s |
| **Ingress Controller** | Traefik | External Access & Load Balancing | âœ… Automatic SSL & Service Discovery |
| **Cloud Platform** | OpenStack | Compute, Network, Storage Resources | âœ… Multi-Cloud Ready |
| **Application Framework** | Streamlit | Web-basierte KI-Anwendung | âœ… Real-time ML Processing |
| **AI/ML** | Google Gemini 2.0 Flash API | Kalorien-SchÃ¤tzung aus Bildern | âœ… Multimodal AI Integration |
| **Containerization** | Docker | Application Packaging | âœ… Multi-stage Builds |
| **Monitoring** | Prometheus + Grafana | Real-time Observability | âœ… HPA Metrics Integration |
| **Autoscaling** | Horizontal Pod Autoscaler | Dynamic Scaling | âœ… CPU + Memory Metrics |
| **Version Control** | Git + Semantic Versioning | Infrastructure & App Versioning | âœ… Zero-Downtime Deployments |
| **Datalake** | MinIO | Datenspeicherung | âœ… Cloud Nativer Datalake |
| **Big Data Processing** | Apache Spark + MLlib | Verteilte Batch-Verarbeitung | âœ… Skalierbare ML-Pipelines |
| **Stream Processing** | Apache Kafka | Echtzeit-Datenverarbeitung | âœ… Horizontale Skalierbarkeit + ML-Integration |

## ğŸš€ Quick Start

### Voraussetzungen

- OpenStack-Zugang mit Administrator-Rechten
- Terraform >= 1.0
- Git
- SSH-Key in OpenStack erstellt
- Docker (optional fÃ¼r lokale Entwicklung)

### Erste Installation

1. **Repository klonen**
   ```bash
   git clone <repository-url>
   cd cloud-computing-vl
   ```

2. **Setup ausfÃ¼hren**
   ```bash
   chmod +x setup.sh
   ./setup.sh
   ```

3. **OpenStack-Credentials konfigurieren**
   ```bash
   # Template zu openrc.sh kopiert - jetzt editieren
   vim openrc.sh
   # Oder: nano openrc.sh
   ```

4. **Terraform-Variablen konfigurieren**
   ```bash
   # Template zu terraform.tfvars kopiert - jetzt editieren  
   vim terraform.tfvars
   # Oder: nano terraform.tfvars
   ```

5. **Credentials laden und deployen**
   ```bash
   source openrc.sh
   ./version-manager.sh deploy v1.0
   ```

6. **Status Ã¼berwachen**
   ```bash
   ./monitor.sh
   ```

### ğŸ†˜ Hilfe bei der Konfiguration

```bash
# Hilfe fÃ¼r OpenStack-Einstellungen
./help.sh

# Template-Dateien anzeigen
cat openrc.sh.template
cat terraform.tfvars.template
```

## ğŸ“‹ Kommandos

### Version Manager

```bash
# ğŸ“‹ Alle verfÃ¼gbaren Versionen anzeigen
./version-manager.sh list

# ğŸš€ Spezifische Version deployen (Standard-Deployment)
./version-manager.sh deploy v1.0

# ğŸ”„ Zero-Downtime Deployment (Produktions-bereit!)
./version-manager.sh zero-downtime v1.1

# ğŸ·ï¸ Neue Version erstellen
./version-manager.sh create v1.1

# âª Rollback zu vorheriger Version
./version-manager.sh rollback v1.0

# ğŸ“ˆ Anwendung skalieren
./version-manager.sh scale 5

# ğŸ“Š Cluster-Status anzeigen
./version-manager.sh status

# ğŸ“‹ Application Logs anzeigen
./version-manager.sh logs

# ğŸ§¹ Infrastructure zerstÃ¶ren
./version-manager.sh cleanup
```

### ğŸ”„ Zero-Downtime Deployment Features

```bash
# Erweiterte Zero-Downtime Deployment
./version-manager.sh zero-downtime v2.0
```

**Features:**
- ğŸ—ï¸ **Parallele Infrastruktur**: Neue Version wird parallel zur alten aufgebaut
- ğŸ¥ **Health Checks**: Automatische GesundheitsprÃ¼fung vor Switch
- ğŸ”„ **Auto-Rollback**: Bei Fehlern automatischer Rollback zur stabilen Version
- ğŸ’¾ **State Backup**: Sichere Sicherung der aktuellen Infrastruktur
- âš¡ **Echter Zero-Downtime**: Keine Unterbrechung des Service

### Monitoring

```bash
# ğŸ” Realtime Cluster Monitoring
./monitor.sh

# ğŸ“Š Einmaliger Status-Check
./version-manager.sh status

# ğŸ“Š Monitoring Dashboard Ã¶ffnen
./version-manager.sh dashboard
```

### ğŸŒ External Access

Nach erfolgreichem Deployment sind folgende Services erreichbar:

```bash
# ğŸ¯ Hauptanwendung (Streamlit)
http://MASTER-IP/           # Ingress (Port 80)
http://MASTER-IP:30001      # NodePort

# ğŸ“Š Monitoring
http://MASTER-IP:30300      # Grafana Dashboard (admin/admin)
http://MASTER-IP:30090      # Prometheus Metrics

# ğŸ” SSH Access
ssh -i ~/.ssh/YOUR-KEY ubuntu@MASTER-IP
```

## ğŸ“Š Scaling Demo & Monitoring Dashboard

### ğŸ¯ Automatisierte Scaling-Demonstration

Das Projekt enthÃ¤lt eine vollstÃ¤ndige Scaling-Demo mit Grafana Dashboard fÃ¼r die Visualisierung der Horizontal Pod Autoscaler (HPA) FunktionalitÃ¤t.

#### Setup des Monitoring Dashboards

```bash
# 1. Dashboard importieren
./version-manager.sh import-dashboard

# 2. Monitoring Dashboard Ã¶ffnen
./version-manager.sh monitoring
```

#### Scaling Demo ausfÃ¼hren

```bash
# VollstÃ¤ndige Scaling Demo mit Dashboard
./scaling-demo.sh
```

**Die Demo fÃ¼hrt automatisch folgende Schritte aus:**

1. **ğŸ“Š Monitoring Setup** - ÃœberprÃ¼fung des Prometheus/Grafana Stacks
2. **ğŸ›ï¸ Dashboard Import** - Automatischer Import des Caloguessr Dashboard
3. **ğŸŒ Browser-Integration** - Ã–ffnet Grafana Dashboard automatisch (macOS)
4. **âš™ï¸ Metrics Server** - Installation/ÃœberprÃ¼fung der Kubernetes Metrics
5. **âš¡ Load Generation** - Konfigurierbare Last-Generierung
6. **ğŸ“ˆ Live Monitoring** - Echzeit-Ãœberwachung der Skalierung

#### Monitoring URLs

Nach dem Deployment sind folgende Monitoring-Services verfÃ¼gbar:

```bash
# Grafana Dashboard (admin/admin)
http://YOUR-MASTER-IP:30300

# Prometheus Metrics
http://YOUR-MASTER-IP:30090

# Caloguessr App
http://YOUR-MASTER-IP:30001
```

#### Dashboard Features

Das **Caloguessr Scaling Demo Dashboard** zeigt:

- ğŸ“Š **Pod Count & HPA Status** - Aktuelle vs. gewÃ¼nschte Replicas
- ğŸ“ˆ **Pod Status Overview** - Live Status aller Pods (Running/Pending/Failed)
- ğŸ’» **CPU Usage per Pod** - CPU-Verbrauch mit HPA-Schwellwerten
- ğŸ§  **Memory Usage per Pod** - Speicher-Verbrauch pro Pod
- ğŸŒ **Network I/O per Pod** - Netzwerk-Traffic wÃ¤hrend Load Tests
- ğŸ”„ **HPA Scaling Events** - Visualisierung der Scaling-Ereignisse

#### Load Test Konfiguration

```bash
# Demo mit benutzerdefinierten Parametern
./scaling-demo.sh

# Interaktive Konfiguration:
# - Load Test Dauer (Standard: 600s = 10 Minuten)
# - Anzahl paralleler Requests (Standard: 100)
```

#### Typischer Scaling-Ablauf

1. **Baseline** - Start mit 2 Pods (HPA Minimum)
2. **Load Increase** - CPU-Last steigt Ã¼ber 10% Schwellwert
3. **Scaling Trigger** - HPA erhÃ¶ht gewÃ¼nschte Replicas
4. **Pod Creation** - Neue Pods werden erstellt
5. **Load Distribution** - Last verteilt sich auf mehr Pods
6. **Scale Down** - Nach Load-Ende: Pods werden reduziert

### ğŸ”§ Monitoring-Kommandos

```bash
# Dashboard importieren/aktualisieren
./version-manager.sh import-dashboard

# Monitoring Dashboard Ã¶ffnen
./version-manager.sh dashboard

# Live-Monitoring starten
./monitor.sh

# Manuelle Skalierung testen
./version-manager.sh scale 8

# HPA Status prÃ¼fen
./version-manager.sh status
```

### ğŸ“Š Metriken & Alerting

Das System Ã¼berwacht automatisch:

- **Pod Metrics**: CPU, Memory, Network I/O
- **Cluster Health**: Node Status, Pod Phases
- **HPA Behavior**: Scaling Events, Target Metrics
- **Application Performance**: Response Times, Error Rates

## ğŸ“± Anwendung

### Caloguessr - KI-Kalorien-SchÃ¤tzer

Die Streamlit-Anwendung ermÃ¶glicht:

- ğŸ“¸ **Bild-Upload** von Lebensmitteln
- ğŸ§  **KI-Analyse** mit Google Gemini API
- ğŸ“Š **Kalorien-SchÃ¤tzung** und NÃ¤hrwert-Analyse
- ğŸŒ **Web-Interface** Ã¼ber Browser
- ğŸ“± **Responsive Design** fÃ¼r alle GerÃ¤te

#### Features

- âœ¨ **Benutzerfreundliche UI** mit Streamlit
- ğŸ” **API-Key Management** Ã¼ber Web-Interface
- ğŸ“ˆ **Detaillierte NÃ¤hrwert-Analyse**
- âš¡ **Schnelle Bildverarbeitung**
- ğŸ” **Genauigkeits-EinschÃ¤tzung**

## ğŸ¯ Versionierung & Deployment

### Semantic Versioning

Das Projekt verwendet [Semantic Versioning](https://semver.org/):

```
v1.0.0 - Initial Release
â”œâ”€â”€ v1.0.1 - Bugfixes
â”œâ”€â”€ v1.1.0 - New Features  
â””â”€â”€ v2.0.0 - Breaking Changes
```

### Entwicklungsworkflow

1. **Ã„nderungen machen**
   ```bash
   # App-Code editieren
   vim app/caloguessr.py
   ```

2. **Neue Version erstellen**
   ```bash
   ./version-manager.sh create v1.1
   ```

3. **Deployen**
   ```bash
   ./version-manager.sh deploy v1.1
   ```

4. **Testen & Verifizieren**
   ```bash
   ./version-manager.sh status
   curl http://<MASTER-IP>:30001
   ```

5. **Bei Problemen: Rollback**
   ```bash
   ./version-manager.sh rollback v1.0
   ```

## ğŸ“Š Monitoring & Debugging

### VerfÃ¼gbare Metriken

- ğŸ“Š **Cluster-Status** (Nodes, Pods, Services)
- ğŸ“ˆ **Resource Usage** (CPU, Memory)
- ğŸ”— **Application Health** (HTTP Response Codes)
- ğŸ“± **Pod-Status** (Running, Pending, Failed)
- ğŸŒ **Service Connectivity** (Internal/External)

### Data Lake / Big Data-Processing

1. **Data Lake Setup ausfÃ¼hren**
   ```bash
   ./version-manager.sh setup-datalake
   ```

2. **Daten in Data Lake laden**
   ```bash
   ./version-manager.sh data-ingestion
   ```

3. **Apache Spark ML Pipeline ausfÃ¼hren**
   ```bash
   ./version-manager.sh spark-ml-pipeline
   ```

4. **Cleanup**
   ```bash
   ./version-manager.sh cleanup-ml-jobs
   ```

**Big Data Workflow:**
- **MinIO Data Lake**: S3-kompatibles Object Storage fÃ¼r groÃŸe DatensÃ¤tze
- **Data Ingestion**: Generiert 50.000+ Food-Samples und lÃ¤dt sie in MinIO
- **Spark ML**: Verteilte Batch-Verarbeitung mit Apache Spark und MLlib
- **Zugriff**: MinIO Console unter `http://MASTER-IP:30901` (minioadmin/minioadmin123)

### Debugging

```bash
#  Application Logs
./version-manager.sh logs

# ğŸ”§ Manuelle SSH-Verbindung (automatisch richtiger SSH-Key)
source openrc.sh
MASTER_IP=$(terraform output -raw master_ip)
SSH_KEY=$(grep "key_pair" terraform.tfvars | cut -d'"' -f2)
ssh -i ~/.ssh/$SSH_KEY ubuntu@$MASTER_IP
```

### HÃ¤ufige Probleme

| Problem | Symptom | LÃ¶sung |
|---------|---------|--------|
| **SSH-Verbindung fehlschlÃ¤gt** | Connection refused | SSH-Key Name in terraform.tfvars prÃ¼fen |
| **SSH-Key nicht gefunden** | Permission denied | PrÃ¼fen ob `~/.ssh/YOUR-KEY` existiert |
| **App startet nicht** | Pods in `Pending` Status | `./version-manager.sh logs` fÃ¼r Details |
| **OpenStack-Fehler** | Authentication failed | openrc.sh Credentials prÃ¼fen |
| **Workers joinen nicht** | Nur Master Node sichtbar | Cloud-init Logs prÃ¼fen |
| **Service nicht erreichbar** | 404/Connection Refused | Security Groups & NodePort prÃ¼fen |

### Big Data Stream Processing

1. **Stream Processing Setup ausfÃ¼hren**
   ```bash
   ./version-manager.sh setup-kafka
   ```

2. **Kafka Topics erstellen**
   ```bash
   ./version-manager.sh create-kafka-topic sensor-data 9 1
   ```

3. **Stream Prozessoren deployen**
   ```bash
   ./version-manager.sh deploy-ml-stream-processor
   ```

4. **Stream Demo starten**
   ```bash
   ./version-manager.sh kafka-stream-demo
   ```

5. **Kafka UI fÃ¼r Monitoring starten**
   ```bash
   ./version-manager.sh deploy-kafka-ui
   ./version-manager.sh open-kafka-ui
   ```

**Stream Processing Workflow:**
- **Apache Kafka Cluster**: Hochperformanter Message Broker fÃ¼r Echtzeit-Streaming
- **Horizontale Skalierbarkeit**: Partitionierte Topics (3, 6, 9 Partitionen) fÃ¼r parallele Verarbeitung
- **Stream Prozessoren**: Mehrere Prozessor-Instanzen fÃ¼r verteilte Event-Verarbeitung
- **ML-basierte Analyse**: Anomalieerkennung und EchtzeitÃ¼berwachung von Sensor-Daten
- **Management Interfaces**: Kafka-UI, Kafdrop und Kafka Manager fÃ¼r umfassendes Monitoring
- **Zugriff**: Kafka Manager unter `http://MASTER-IP:30900`, Kafka-UI unter `http://MASTER-IP:30902`

## ğŸ“š WeiterfÃ¼hrende Dokumentation

### Interne Dokumentation

- [`setup.sh`](./setup.sh) - Erstinstallation und Konfiguration
- [`help.sh`](./help.sh) - Hilfe fÃ¼r OpenStack-Einstellungen  
- [`version-manager.sh`](./version-manager.sh) - Hauptskript fÃ¼r Version Management
- [`monitor.sh`](./monitor.sh) - Cluster Monitoring
- [`scaling-demo.sh`](./scaling-demo.sh) - Automatisierte Scaling-Demo
- [`cloud-init-master.tpl`](./cloud-init-master.tpl) - Master Node Konfiguration
- [`main.tf`](./main.tf) - Terraform Infrastructure Definition
- [`grafana-dashboard-caloguessr.json`](./grafana-dashboard-caloguessr.json) - Grafana Dashboard Konfiguration
- [`datalake.yaml`](./big-data/datalake.yaml) - Datalake MinIO Konfiguration
- [`data-ingestion-job.yaml`](./big-data/data-ingestion-job.yaml) - Data Ingestion Konfiguration
- [`spark-ml-pipeline-job.yaml`](./big-data/spark-ml-pipeline-job.yaml) - Batch Verarbeitung Konfiguration

### Template-Dateien

- [`openrc.sh.template`](./openrc.sh.template) - OpenStack Credentials Template
- [`terraform.tfvars.template`](./terraform.tfvars.template) - Terraform Variables Template

### Externe Ressourcen

- [Terraform OpenStack Provider](https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs)
- [K3s Documentation](https://docs.k3s.io/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [MinIO Documentation](https://www.min.io/)
- [Apache Spark Documentation](https://spark.apache.org/mllib/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)

## ğŸ“„ Lizenz

Dieses Projekt steht unter der MIT-Lizenz.

---

## ğŸ¯ Projekt-Status

```bash
# Aktueller Status prÃ¼fen
./version-manager.sh status

# Letzte Version anzeigen  
./version-manager.sh list

# Monitoring Dashboard Ã¶ffnen
./version-manager.sh dashboard

# Scaling Demo starten
./scaling-demo.sh
``` 
